{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification,TrainingArguments,Trainer, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pd.read_csv(\"train_data.csv\", sep='|')\n",
    "validate_data = pd.read_csv(\"validate_data.csv\", sep='|')\n",
    "test_data = pd.read_csv(\"test_data.csv\", sep='|')\n",
    "\n",
    "display(train_data)\n",
    "display(validate_data)\n",
    "display(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "validate_data = validate_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training size: {}\".format(len(train_data)))\n",
    "print(\"Validation size: {}\".format(len(validate_data)))\n",
    "print(\"Testing size: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escolhendo a GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", from_tf=True)\n",
    "model.config.num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desabilitando a atualização dos gradientes para acelerar o processamento e diminuir o consumo\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Comentar esta seção para ativar o ajuste de gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a arquitetura da camada classificadora\n",
    "# composta por várias camadas densas e de ativação para processamento não linear e uma camada Softmax \n",
    "# para obter as probabilidades de cada classe.\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(768, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 2),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "# Alocando o modelo e o critério de perda na GPU\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  #limpando String para caso a fonte de dados não seja pre-processada\n",
    "  return text.replace('\\n', ' ').replace('\\t', ' ').replace('   ', ' ').replace('  ', ' ')\n",
    " \n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    parts = []\n",
    "\n",
    "    text_len = len(text.split(' '))\n",
    "    delta = 300 \n",
    "    max_parts = 5 \n",
    "    nb_cuts = int(text_len / delta)\n",
    "    nb_cuts = min(nb_cuts, max_parts)\n",
    "    \n",
    "    \n",
    "    for i in range(nb_cuts + 1):\n",
    "        text_part = ' '.join(text.split(' ')[i * delta: (i + 1) * delta])\n",
    "        parts.append(tokenizer.encode(text_part, return_tensors=\"pt\", max_length=500).to(device))\n",
    "\n",
    "    return parts\n",
    "\n",
    "def testText(text):\n",
    "    text_parts = preprocess_text(text)\n",
    "    overall_output = torch.zeros((1,2)).to(device)\n",
    "    try:\n",
    "        for part in text_parts:\n",
    "            if len(part) > 0:\n",
    "                overall_output += model(part.reshape(1, -1))[0]\n",
    "    except RuntimeError:\n",
    "        print(\"out of memory\")\n",
    "\n",
    "    overall_output = F.softmax(overall_output[0], dim=-1)\n",
    "\n",
    "    value, result = overall_output.max(0)\n",
    "\n",
    "    term = \"falso\"\n",
    "    if result.item() == 0:\n",
    "        term = \"verdadeiro\"\n",
    "\n",
    "    print(\"{} : {}%\".format(term, value.item() * 100))\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertFakeNewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_seq_length=512):\n",
    "        self._labels = torch.tensor(df[\"label\"].values, dtype=torch.long)\n",
    "        self._encodings = {\"input_ids\": [],\n",
    "                           \"token_type_ids\": [],\n",
    "                           \"attention_mask\": []}\n",
    "\n",
    "        for txt in df[\"text\"].values:\n",
    "            enc_dict = tokenizer.encode_plus(\n",
    "                text=txt,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_seq_length,\n",
    "                return_token_type_ids=True,\n",
    "                padding=\"max_length\",\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "            )\n",
    "            \n",
    "            if len(enc_dict[\"input_ids\"][0]) > max_seq_length:\n",
    "                print(\"Truncamento detectado após adição de tokens especiais!\")\n",
    "\n",
    "            for k, v in enc_dict.items():\n",
    "                self._encodings[k].append(v[0])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: value[idx] for key, value in self._encodings.items()}\n",
    "        item[\"labels\"] = self._labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataTK = BertFakeNewsDataset(train_data,tokenizer)\n",
    "validate_dataTK = BertFakeNewsDataset(validate_data,tokenizer)\n",
    "test_dataTK = BertFakeNewsDataset(test_data,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1_score\": f1_score(labels, predictions, average='weighted'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir='C:\\TCC',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2.5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=5e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_dataTK,\n",
    "    eval_dataset=validate_dataTK,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(train_dataTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(validate_dataTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataTK) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falso = \"\"\"Mais uma pérola da \"senhora que estoca vento\". Em nota, ela sugere que a PF \"tortura\" investigados.  A ex-presidente impeachmada Dilma Rousseff continua falando pelos cotovelos.  (observação: Em seu texto, ela não usou a palavra \"tortura\" [...] mas para quem sabe ler, um pingo é letra!) Durante a última semana, Dilma teve a audácia (a petulância, a ousadia, destemor) de dizer que os investigadores da Polícia Federal estão ameaçando os delatores da Lava-Jato. Dilma também atacou o ministro relator da Lava-Jato, Edson Fachin, e o acusou de abrir as delações de João Santana e Mônica Moura para a imprensa antes sem antes dar acesso dos documentos a seus advogados. Resumindo, Dilma é honesta [...] já Edson Fachin (Ministro do STF) e a Polícia Federal são os \"bandidos da história\" Em uma nota divulgada na última sexta-feira, a \"senhora que estoca vento\" também disse que todos os delatores são mentirosos. abaixo a íntegra da nota: 1. Infelizmente, chega tarde a decisão do relator da Lava Jato no Supremo Tribunal Federal, ministro Edson Fachin, suspendendo o sigilo dos depoimentos de João Santana e Monica Moura. 2. Há semanas, a defesa da presidenta eleita Dilma Rousseff havia feito tal pedido ao Tribunal Superior Eleitoral, a fim de apresentar suas alegações finais ao relator do caso das contas de campanha, ministro Herman Benjamin. 3. A defesa foi prejudicada pela negativa do relator. Não foi possível cotejar os depoimentos prestados pelo casal à Justiça Eleitoral e na Lava Jato. 4. As contradições e falsos testemunhos foram vislumbrados, apesar disso, pelo que foi divulgado amplamente pela imprensa, na velha estratégia do vazamento seletivo dos depoimentos – uma rotina nos últimos tempos. 5. Agora mesmo, os depoimentos são entregues à imprensa, mas não repassados oficialmente à defesa da presidente eleita. 6. Dilma Rousseff, contudo, reitera o que apontou antes: João Santana e Monica Moura prestaram falso testemunho e faltaram com a verdade em seus depoimentos, provavelmente pressionados pelas ameaças dos investigadores. 7. Apesar de tudo, a presidente eleita acredita na Justiça e sabe que a verdade virá à tona e será restabelecida.\"\"\"\n",
    "\n",
    "Verdadeiro = \"\"\"Saúde de Lula é 'excelente', atestam exames feitos hoje\n",
    "\n",
    "O ex-presidente Luiz Inácio Lula da Silva encontra-se em \"excelente condição de saúde e sem qualquer evidência de neoplasia\", diz o boletim assinado pelos médicos Antonio Carlos Onofre de Lira, diretor técnico médico, e Paulo Cesar Ayroza Galvão, diretor clínico do hospital sírio-libanês.\n",
    "\n",
    "Lula foi submetido a exames clínicos, laboratoriais, PET-CT, ressonância nuclear magnética e laringoscopia durante toda a manhã deste sábado, 1. Todos de rotina. Essa rotina vem sendo repetida desde 2011, quando foi detectado um câncer na laringe do ex-presidente, que se submeteu a tratamento e deve fazer o controle por cinco anos. O último exame havia sido realizado em 10 de agosto. As equipes que acompanham o ex-presidente são coordenadas pelos médicos Roberto Kalil Filho e Artur Katz. Lula deixou o hospital sem falar com a imprensa.\n",
    "\"\"\"\n",
    "\n",
    "testText(falso)\n",
    "testText(Verdadeiro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'modelo.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "model.push_to_hub(\"\",use_auth_token='')\n",
    "\n",
    "tokenizer.push_to_hub(\"\",use_auth_token='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
